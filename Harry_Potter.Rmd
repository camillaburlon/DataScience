---
title: "Harry Potter"
author: "Camilla Burlon"
output: ioslides_presentation
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, message = FALSE, warning = FALSE)
```

##HARRY POTTER
```{r echo=FALSE, eval=TRUE}
#Pacchetto contenente i 7 libri della seria Harry Potter di JK Rowling. (Github)
#if (packageVersion("devtools") < 1.6) {
#  install.packages("devtools")
#}
#devtools::install_github("bradleyboehmke/harrypotter")

library(tidyverse) 
library(tidytext)
library(stringr)
library(harrypotter) 
library(igraph)
library(tidygraph)
library(ggraph)
library(topicmodels)
library(widyr)


books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban, goblet_of_fire,
              order_of_the_phoenix, half_blood_prince, deathly_hallows)
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban", "Goblet of Fire", 
            "Order of the Phoenix", "Half-Blood Prince", "Deathly Hallows")

HP_books <- tibble()

#libri in un tibble
for(i in seq_along(titles)) {
  
  tmp <- tibble(chapter = seq_along(books[[i]]), text = books[[i]]) %>%
             mutate(book = titles[i])
  HP_books <- rbind(HP_books, tmp)
}
HP_books$book <- factor(HP_books$book, levels = rev(titles))

```

## Divisione in capitoli
```{r}
#capitoli per libro
HP_books %>% 
  group_by(book) %>% 
  summarise(chapters = n())

# tokenization: tutte le parole dei libri 
tidy_HPbooks <- unnest_tokens(tbl = HP_books, output = word, input = text)
tidy_HPbooks

#rimozione delle stopword
tidy_HPbooks <- tidy_HPbooks %>%
  anti_join(stop_words) 

```

## Word frequency 
```{r}
# grafico delle frequenze
tidy_HPbooks %>%
  count(word, sort = TRUE) %>%
  filter(n > 1200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

##Grafico per la frequenza delle parole
```{r}
library(wordcloud)

tidy_HPbooks %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))
```

##Grafici per i vari libri
```{r}
tidy_HPbooks %>%
  group_by(book) %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(book = factor(book, levels = titles), text_order = nrow(.):1, 
         word = reorder(word, text_order)) %>% #metto dei numeri ai libri
  ggplot(aes(word, n, fill = book)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ book, scales = "free_y") +
    labs(title = "Parole pi√π frequenti in Harry Potter", x = NULL, y = NULL) +
    coord_flip() 
```

##Sentiment analysis   
```{r}
HP_sentiment <- tidy_HPbooks %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, chapter, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

HP_sentiment

#per ogni libro esprime un giudizio per i vari capitoli 
ggplot(HP_sentiment, aes(chapter, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

#parole best per ogni sentimento
bing_word <- tidy_HPbooks %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word

bing_word %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") + 
  labs(y = "Parole piu frequenti per il sentimento negativo e quello positivo", x = NULL) +
  coord_flip() 

#parole di gioia
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

#filtro tutte le parole che esprimono gioia, contandole
tidy_HPbooks %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

#parole classificate come paura 
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_HPbooks %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

#parole classificate come tristi
nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

tidy_HPbooks %>%
  inner_join(nrc_sadness) %>%
  count(word, sort = TRUE)

```

#Bigrammi e Trigrammi
```{r}
#tibble dei bigrammi
HP_bigrams <- HP_books %>%
  unnest_ngrams(bigram, text, ng = "ngrams", n = 2) 

# remove stop words
HPbigrams_separated <- HP_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

HPbigrams_filtered <- HPbigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#bigram counts
bigram_counts <- HPbigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
bigram_counts

#occorrenze di lord voldemort vs harry potter
bigram_voldemortVSharry <- HPbigrams_filtered %>% 
  filter((word1 == "lord" & word2 == "voldemort") | (word1 == "harry" & word2 == "potter")) %>%
  count(word1, word2, sort = TRUE) %>%
  top_n(2)
bigram_voldemortVSharry

bigram_graph <- bigram_counts %>%
  filter(n > 50) %>% 
  as_tbl_graph() 

a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

# plot the graph
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

#cluster per il mio prefe
hp_section_words <- tidy_HPbooks %>%
  filter(book == "Half-Blood Prince") %>%
  mutate(section = row_number() %/% 10) %>% 
  filter(section > 0) 

hp_section_words

word_pairs <- hp_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs

word_cors <- hp_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

word_cors %>%
  filter(correlation > .15) %>%
  as_tbl_graph() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

#occorrenze di "tu sai chi" - trigramma
HP_bigrams3 <- HP_books %>%
  unnest_ngrams(bigram, text, n = 3) 
HPbigrams_separated3 <- HP_bigrams3 %>%
  separate(bigram, c("word1", "word2", "word3"), sep = " ")

bigram_voldemort <- HPbigrams_separated3 %>% 
  filter(word1 == "you" & word2 == "know" & word3 == "who") %>%
  count(word1, word2, word3, sort = TRUE)
  
bigram_voldemort
```
##topics

```{r}
#conteggio parole
wordHP = tidy_HPbooks %>%
  anti_join(stop_words) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

hp_dtm = wordHP  %>%
  cast_dtm(book, word, n)

hp_lda <- LDA(hp_dtm, k = 7, control = list(seed = 1234))

hp_topics <- tidy(hp_lda, matrix = "beta")
hp_topics

hp_top_terms <- hp_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#grafico
hp_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

```





